{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Install required packages\n",
    "    !pip install transformers datasets torch bitsandbytes-cuda111 peft # Replace 'cuda111' with your CUDA version\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Import Libraries\n",
    "    from transformers import GPT2Tokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "    from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "    from datasets import load_dataset, load_metric\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Load and Evaluate a Pre-trained Model\n",
    "\n",
    "    ## a. Load Model and Tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70943f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## b. Load and Preprocess IMDb Dataset\n",
    "    dataset = load_dataset(\"imdb\", split='train[:1%]')\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': [label - 1 for label in examples['label']]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## c. Evaluate the Pretrained Model\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    def evaluate_model(model, data_loader):\n",
    "        model.eval()\n",
    "        total_acc = 0\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to('cuda')\n",
    "            labels = batch['labels'].to('cuda')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            acc = (predictions == labels).float().mean()\n",
    "            total_acc += acc.item()\n",
    "        return total_acc / len(data_loader)\n",
    "\n",
    "    data_loader = DataLoader(tokenized_dataset, batch_size=8)\n",
    "    pretrained_model_acc = evaluate_model(model, data_loader)\n",
    "    print(\"Accuracy of Pretrained Model:\", pretrained_model_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172790c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Perform Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "    ## a. Creating a LoRA Config\n",
    "    lora_config = LoraConfig()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f52b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## b. Convert to a PEFT Model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## c. Train the PEFT Model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad82528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## d. Save the Trained Model\n",
    "    peft_model.save_pretrained(\"./peft_model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Perform Inference Using the Fine-Tuned Model and Compare Performance\n",
    "    fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./peft_model\")\n",
    "    fine_tuned_model_acc = evaluate_model(fine_tuned_model, data_loader)\n",
    "    print(\"Fine-Tuned Model Accuracy:\", fine_tuned_model_acc)\n",
    "    print(\"Pretrained Model Accuracy:\", pretrained_model_acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
